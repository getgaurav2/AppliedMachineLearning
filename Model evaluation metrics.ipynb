{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics for comparing prediction error in classification and regression models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Classification Metrics: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "# Metrics module allows you define particular metrics\n",
    "\n",
    "# Accuracy\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "y_pred = [0, 1, 1, 1]\n",
    "y_true = [1, 1, 1, 0] # note that we are comparing predictions from a model to some ytest values\n",
    "\n",
    "print(accuracy_score(y_true, y_pred)) # proportion of ytest predicted correctly\n",
    "\n",
    "\n",
    "print(accuracy_score(y_true, y_pred, normalize=False))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter of interest:\n",
    "\n",
    "normalize : default=True\n",
    "\n",
    "    If False, return the number of correctly classified samples. Otherwise, return the fraction of correctly classified samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2 0]\n",
      " [1 3]]\n"
     ]
    }
   ],
   "source": [
    "#The confusion_matrix function can also evaluate classification accuracy by computing the confusion matrix.\n",
    "\n",
    "# It's simply a tabulation of predicted y on test y.\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "y_true = [1, 0, 1, 1, 0, 1]\n",
    "y_pred = [0, 0, 1, 1, 0, 1]\n",
    "\n",
    "mat = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "print(mat)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xbec6e10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "\n",
    "sns.heatmap(mat, square=True, annot=True)\n",
    "plt.xlabel('true label')\n",
    "plt.ylabel('predicted label');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In the above we see true positives or TP =  3, false positives or  FP = 1, true negatives or TN = 2, and false negatives or FN = 0. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above we see true positives or TP =  3, false positives or  FP = 1, true negatives or TN = 2, \n",
    "and false negatives or FN = 0.  \n",
    "\n",
    "Accuracy = TP + TN / TP + TN +FP +FN\n",
    "\n",
    "Sometimes measuring accuracy alone can give use to much confidence in our predictive models.  If we have misbalanced data such that 1000 values are labeled one to 5 labeled zero, then a simple prediction of all ones could give us unreasonably high confidence in the success of our model.\n",
    "\n",
    "In predictive analytic classification models we tend to use the f1 score or the ROC area under the curve instead.\n",
    "\n",
    "These metrics account for imbalanced data better because they measure each of the above classification categories against each other in ways that take imbalanced data into account.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 0, 1, 3)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true = [1, 0, 1, 1, 0, 1]\n",
    "y_pred = [0, 0, 1, 1, 0, 1]\n",
    "tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "tn, fp, fn, tp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the classification report to understand predictive success"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 0, 1, 2)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_true = [0, 1, 1, 1, 0]\n",
    "y_pred = [0, 0, 1, 1, 0]\n",
    "tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "tn, fp, fn, tp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "    class 0       0.67      1.00      0.80         2\n",
      "    class 1       1.00      0.67      0.80         3\n",
      "\n",
      "avg / total       0.87      0.80      0.80         5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "target_names = ['class 0', 'class 1']\n",
    "print(classification_report(y_true, y_pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining precision:\n",
    "The precision is the ratio tp / (tp + fp) where tp is the number of true positives and fp the number of false positives. The precision is intuitively the ability of the classifier not to label as positive a sample that is negative.\n",
    "\n",
    "i.e.-precision gives us information about its performance with respect to false positives.  If we want to minimize false positives then we want precision to be 1.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining recall:\n",
    "\n",
    "The recall is the ratio tp / (tp + fn) where tp is the number of true positives and fn the number of false negatives. The recall is intuitively the ability of the classifier to find all the positive samples.\n",
    "\n",
    "i.e.-if we want to focus more on minimizing False Negatives, we would want our Recall to be closer to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the f1 score:\n",
    "\n",
    "The F1 score can be interpreted as a weighted average of the precision and recall, where an F1 score reaches its best value at 1 and worst score at 0. The relative contribution of precision and recall to the F1 score are equal. The formula for the F1 score is:\n",
    "\n",
    "F1 = 2 * (precision * recall) / (precision + recall) \n",
    "\n",
    "if both prec. and rec are high, then f1 is high.  BUT\n",
    "if one or both are low, then f1 reacts to the signal and it drops a lot.\n",
    "                                                      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The ROC Area under the Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xe0e4358>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_true = [0, 1, 1, 1, 0]\n",
    "y_pred = [0, 0, 1, 1, 0]\n",
    "\n",
    "mat = confusion_matrix(y_true, y_pred)\n",
    "sns.heatmap(mat, square=True, annot=True)\n",
    "plt.xlabel('true label')\n",
    "plt.ylabel('predicted label');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8333333333333333\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "print(roc_auc_score(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is the logic for the ROC area under the curve?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](https://upload.wikimedia.org/wikipedia/commons/thumb/3/36/ROC_space-2.png/800px-ROC_space-2.png \"AUC\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is a plot of the True Positive Rate (on the y-axis) versus the False Positive Rate (on the x-axis) for every possible classification threshold.  \n",
    "\n",
    "We treat the FPR as X and TPR as Y to plot points at a large range of threshold points on the x axis.  (i.e. We usually define success for y at a cut point of .5.  Here we do it for a large range of cut points. Then we plot points for each X,Y TPR and FPR value.\n",
    "\n",
    "The area under the curve is calculated using calculus.  We find a value from .5 to 1, with 1 meaning perfect prediction.  \n",
    "\n",
    "Why should we use AUC instead of Accuracy?\n",
    "\n",
    "It accounts for imbalanced data by testing against many threshold definitions!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining your scoring strategy from metric functions\n",
    "\n",
    "Some scoring metrics are built in to cross_val_score and gridsearchCV.\n",
    "\n",
    "Others require you to build them first before they can be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import log_loss, make_scorer\n",
    "log_loss_scorer = make_scorer(log_loss)\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "grid = GridSearchCV(LogisticRegression(), param_grid={'C': [1, 10]}, scoring=log_loss)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Regression metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: some of these values are made negative in sklearn to keep computation consistant.  You will see negative MSE rather than MSE.  Why?  Coders behind sklearn want to stick to one rule: Greater is better.  So they try to make metrics follow this rule.  Greater negative MSE, for example, means better prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mean squared error:\n",
    "\n",
    "![alt text](http://scikit-learn.org/stable/_images/math/44f36557fef9b30b077b21550490a1b9a0ade154.png \"MSE\")\n",
    "\n",
    "Corrects RSS for number of observations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.375"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "y_true = [3, -0.5, 2, 7]\n",
    "y_pred = [2.5, 0.0, 2, 8]\n",
    "mean_squared_error(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Root mean squared error:\n",
    "\n",
    "Take the square root of the above.\n",
    "\n",
    "A bit more robust to outliers than MSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mean absolute error:\n",
    "    \n",
    "    Similar to above, but take absolute value instead of squaring difference between yi and yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "y_true = [3, -0.5, 2, 7]\n",
    "y_pred = [2.5, 0.0, 2, 8]\n",
    "mean_absolute_error(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Median absolute error:\n",
    "    \n",
    "   ![alt text](http://scikit-learn.org/stable/_images/math/9252f9de0d8c2043cf34a26e6f2643a6e66540b9.png \"MedAE\")\n",
    "   \n",
    "   Find median of difference between y and yhat\n",
    "   \n",
    "   Also robust to outliers in data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import median_absolute_error\n",
    "y_true = [3, -0.5, 2, 7]\n",
    "y_pred = [2.5, 0.0, 2, 8]\n",
    "median_absolute_error(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "R-square:\n",
    "       ![alt text](http://scikit-learn.org/stable/_images/math/bdab7d608c772b3e382e2822a73ef557c80fbca2.png \"r2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9486081370449679"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "y_true = [3, -0.5, 2, 7]\n",
    "y_pred = [2.5, 0.0, 2, 8]\n",
    "r2_score(y_true, y_pred) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Can you evaluate the boston dataset using new regression metrics?\n",
    "How about the breast cancer dataset?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
